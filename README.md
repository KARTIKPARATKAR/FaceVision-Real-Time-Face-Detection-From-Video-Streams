# ü§ñ FaceVision ‚Äì Real-Time Face Detection from Video Streams üé•

## üìå [Final Notebook with Results on Kaggle](https://www.kaggle.com/code/kartikparatkar/facevision-real-timefacedetectionfromvideostreams?scriptVersionId=234191458)

---

## üéØ Aim  
To build a Deep Learning model that processes **video as input** and draws a **bounding box** üü• around the face(s), regardless of their position in the video.

---

## üì∏ Data Collection  
We collected **387 images** using a laptop webcam üíª to train the deep learning model effectively.

---

## üóÇÔ∏è Data Setup & Annotation  
- Uploaded the images to **Kaggle** üìÅ  
- Created a `labels/` directory for storing label files  
- Used [`labelImg`](https://github.com/HumanSignal/labelImg) üè∑Ô∏è for annotating images (bounding boxes around faces)  
- Output format was **XML**, later converted to **JSON** for compatibility with Albumentations  
- Dataset Split:
  - üîπ 70% ‚Üí Training (268 images)
  - üîπ 15% ‚Üí Testing (57 images)
  - üîπ 15% ‚Üí Validation (62 images)

üìΩÔ∏è [Labeling Tutorial](https://www.youtube.com/watch?v=fjynQ9P2C08)

---

## üîÑ Data Augmentation using Albumentations  
**Albumentations** is a powerful and fast image augmentation library used for:
- Rotation üîÅ
- Flipping ‚ÜîÔ∏è
- Blurring üå´Ô∏è
- Adding noise üéöÔ∏è  
These augmentations improve the model‚Äôs ability to generalize in real-world scenarios.

---

## üß† Model Building with Keras Functional API  
Using **Keras Functional API**, we:
- Load **VGG16** pre-trained on **ImageNet**
- Freeze the classification head ‚ùÑÔ∏è
- Add two custom heads:
  - üü¢ Classification ‚Äì detects presence of a face
  - üî¥ Regression ‚Äì predicts bounding box coordinates

---

## üß± VGG16 Model Architecture

![VGG16 Model Architecture](https://github.com/KARTIKPARATKAR/FaceVision-Real-Time-Face-Detection-From-Video-Streams/blob/main/VGG16_Model.jpg)

---

## üèóÔ∏è Custom CNN Architecture Overview  
Here's how the custom CNN model is built:
- üì• Input: `(120, 120, 3)`
- üîÑ Pass through VGG16 base
- üîÅ Two branches:
  - `F1` ‚û°Ô∏è Classification Head
    - Dense Layer ‚Üí `2048` nodes + ReLU
    - Output: `class2`
  - `F2` ‚û°Ô∏è Regression Head
    - Dense Layer ‚Üí `2048` nodes + ReLU
    - Output: `regress2`
- üîó Merge both outputs into a unified model

## üß¨ Custom CNN Model Diagram

![Custom CNN Model](https://github.com/KARTIKPARATKAR/FaceVision-Real-Time-Face-Detection-From-Video-Streams/blob/main/facetracker_model.png)

---

## ‚öôÔ∏è Training Strategy & Loss Functions  
- **Classification Loss**: `Binary Crossentropy`
- **Regression Loss**: Custom **Localization Loss** üßÆ ‚Äì penalizes based on deviation from ground-truth bounding boxes

Implemented a custom `FaceTracker` class in Keras:
- `train_step()` ‚Äì Forward pass, compute losses, backpropagation üîÅ
- `test_step()` ‚Äì Forward pass, compute test loss üß™
- `compile()` ‚Äì Combines classification and localization loss üß©
- `call()` ‚Äì Defines model behavior on execution üìû

Used **Adam optimizer** with learning rate decay and trained the model using `.fit()` for **10 epochs** üèãÔ∏è‚Äç‚ôÇÔ∏è

---

## üìä Training Metrics & Evaluation  
Generated the following plots:
- üìâ Total Loss vs Epochs
- üìâ Classification Loss vs Epochs
- üìâ Regression Loss vs Epochs

---

## üß™ Real-Time Testing with Video üé•  
- Captured a **1-minute video** using a webcam üé¶  
- Uploaded the video to Kaggle directory  
- Extracted frames every **0.3 seconds** ‚è±Ô∏è  
- Passed each frame through the model  
- Drew bounding boxes üü• around detected faces  
- Displayed sampled frames with **annotations** (bounding boxes + class labels)

> ‚úÖ *Annotations* here refer to **visual cues** like bounding boxes drawn over images, indicating model predictions.

---

Feel free to ‚≠ê this repo if you found it useful!
